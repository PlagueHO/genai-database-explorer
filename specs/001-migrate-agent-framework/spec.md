# Feature Specification: Migrate from Semantic Kernel to Microsoft Agent Framework

**Feature Branch**: `001-migrate-agent-framework`  
**Created**: 2026-02-20  
**Status**: Draft  
**Input**: User description: "Migrate from Semantic Kernel + Prompty to Microsoft Agent Framework with custom prompt template loader as per the migration plan"

## User Scenarios & Testing *(mandatory)*

### User Story 1 - AI-Enriched Database Schema Descriptions Continue Working (Priority: P1)

As a database analyst, I want to enrich my extracted semantic model with AI-generated descriptions for tables, views, and stored procedures so that I can understand the purpose and meaning of database objects — and this must continue working identically after the underlying AI framework is replaced.

**Why this priority**: This is the primary AI-powered feature of the application. If AI enrichment breaks, the core value proposition is lost. Every other feature depends on the AI pipeline producing correct results.

**Independent Test**: Run `enrich-model` against an existing extracted semantic model and verify that AI-generated descriptions are produced for all tables, views, and stored procedures with equivalent quality and completeness to the current Semantic Kernel implementation.

**Acceptance Scenarios**:

1. **Given** an extracted semantic model with tables, views, and stored procedures, **When** the user runs `enrich-model`, **Then** AI-generated descriptions are produced for each database object using the same prompt templates as before.
1. **Given** a prompt template containing Liquid variable substitution (e.g., `{{project_description}}`), **When** the enrichment process renders the template, **Then** all variables are correctly substituted in the generated prompt.
1. **Given** a prompt template containing Liquid loop syntax (e.g., `{% for table in tables %}`), **When** the enrichment process renders the template, **Then** all loop iterations are correctly expanded in the generated prompt.
1. **Given** a prompt template with few-shot examples (user/assistant message pairs), **When** the prompt is rendered, **Then** the examples are included in the correct order with correct role assignments (system, user, assistant).
1. **Given** an enrichment operation completes, **When** the result is returned, **Then** token usage statistics (input tokens, output tokens) are tracked and logged as before.
1. **Given** a previously enriched semantic model generated by the pre-migration version, **When** the user loads or operates on the model after migration, **Then** all existing descriptions, metadata, and vector data remain fully usable without re-enrichment or re-generation.

---

### User Story 2 - Data Dictionary Import Continues Working (Priority: P1)

As a database analyst, I want to import and apply data dictionary information to my semantic model using AI-powered structured output so that column metadata from external sources is correctly mapped to my model — and this must continue working identically after migration.

**Why this priority**: Data dictionary import relies on AI structured output (JSON schema responses) which is a distinct code path from free-text description generation. It must be validated independently.

**Independent Test**: Run `data-dictionary` with a sample data dictionary source and verify structured output is correctly parsed into the expected format.

**Acceptance Scenarios**:

1. **Given** a data dictionary source file and a target semantic model, **When** the user runs `data-dictionary`, **Then** AI-generated structured output maps dictionary entries to model objects correctly.
1. **Given** the AI service returns a structured JSON response, **When** the response is deserialized, **Then** the result matches the expected schema (e.g., `TableDataDictionary`) without data loss.
1. **Given** the data dictionary operation completes, **When** the result is returned, **Then** token usage statistics are tracked and logged.

---

### User Story 3 - Vector Embedding Generation Continues Working (Priority: P1)

As a database analyst, I want to generate vector embeddings for my semantic model objects so that natural language queries can find relevant database objects — and this must continue working after the embedding generator is migrated.

**Why this priority**: Vector embeddings enable the natural language query feature. Without working embeddings, the query pipeline is broken.

**Independent Test**: Run `generate-vectors` against an enriched semantic model and verify that vector embeddings are produced for all model objects.

**Acceptance Scenarios**:

1. **Given** an enriched semantic model, **When** the user runs `generate-vectors`, **Then** vector embeddings are generated for each database object.
1. **Given** the embedding generation completes, **When** the results are stored, **Then** the embeddings are in the same format and dimensionality as the current implementation.

---

### User Story 4 - CLI Interface Remains Unchanged (Priority: P2)

As a user of the GenAI Database Explorer CLI, I want all existing commands (`init-project`, `extract-model`, `enrich-model`, `query-model`, `export-model`, `data-dictionary`, `generate-vectors`, `show-object`) to work exactly as before so that my workflows and scripts are not disrupted by the internal framework migration.

**Why this priority**: Users interact exclusively through the CLI. Any change to command names, options, or output format would break existing user workflows.

**Independent Test**: Run each CLI command with the same arguments as before and verify identical behavior, exit codes, and output structure.

**Acceptance Scenarios**:

1. **Given** any existing CLI command and its arguments, **When** the user runs the command, **Then** the command executes with the same behavior, exit codes, and output format as before the migration.
1. **Given** the `settings.json` project configuration, **When** the user upgrades to the migrated version, **Then** any new required configuration fields are clearly documented in a migration guide, and existing optional fields continue to be recognized where applicable.

---

### User Story 5 - Prompt Templates Are Preserved (Priority: P2)

As a developer maintaining the application, I want all existing prompt template content to be preserved and reusable so that the quality of AI-generated outputs does not regress after migration.

**Why this priority**: The prompt templates contain carefully crafted system prompts, few-shot examples, and template structures that directly determine AI output quality. Losing or corrupting these would degrade results.

**Independent Test**: Compare the rendered output of each prompt template (with identical input variables) between the old Semantic Kernel Prompty pipeline and the new custom parser/renderer, and verify they are identical.

**Acceptance Scenarios**:

1. **Given** each of the 6 existing prompt template files, **When** the template is loaded by the new parser, **Then** the YAML frontmatter metadata (name, description, model parameters) is correctly extracted.
1. **Given** each prompt template, **When** the template is rendered with the same input variables, **Then** the rendered output is character-for-character identical to what the previous Semantic Kernel Prompty pipeline produced.
1. **Given** the prompt templates are relocated and renamed, **When** a developer looks for prompt files, **Then** they are found in a clearly named directory with a clear file extension.

---

### User Story 6 - Authentication Methods Continue Working (Priority: P2)

As a system administrator, I want both Entra ID (managed identity) and API key authentication to continue working for connecting to AI services so that deployment flexibility is maintained.

**Why this priority**: Different deployment environments use different authentication methods. Both must work to avoid locking out existing users.

**Independent Test**: Configure the application with Entra ID credentials and verify AI operations succeed; then configure with API key credentials and verify the same operations succeed.

**Acceptance Scenarios**:

1. **Given** the application is configured with Entra ID (managed identity) credentials, **When** AI operations are invoked, **Then** authentication succeeds and operations complete normally.
1. **Given** the application is configured with API key credentials, **When** AI operations are invoked, **Then** authentication succeeds and operations complete normally.
1. **Given** the application is configured with missing or invalid credentials, **When** AI operations are invoked, **Then** a clear, actionable error message is displayed.

---

### User Story 7 - Semantic Kernel Dependencies Are Fully Removed (Priority: P3)

As a developer maintaining the application, I want all Semantic Kernel package dependencies and associated code to be completely removed so that the codebase has no legacy framework remnants and future maintenance is simplified.

**Why this priority**: Retaining unused dependencies increases maintenance burden, binary size, and potential versioning conflicts. Clean removal is necessary but lower priority than functional correctness.

**Independent Test**: Verify that no Semantic Kernel NuGet packages remain in any `.csproj` file, no `using Microsoft.SemanticKernel.*` statements exist in source code, and no `#pragma warning disable SKEXP*` suppressions remain.

**Acceptance Scenarios**:

1. **Given** the migration is complete, **When** the project dependencies are inspected, **Then** no Semantic Kernel NuGet packages are referenced.
1. **Given** the migration is complete, **When** the source code is searched, **Then** no `using Microsoft.SemanticKernel` namespaces (except `Microsoft.SemanticKernel.Connectors.InMemory` in vector store files), `KernelArguments`, `PromptExecutionSettings`, or `SKEXP` warning suppressions are found.
1. **Given** the migration is complete, **When** the old `ISemanticKernelFactory` and `SemanticKernelFactory` files are checked, **Then** they have been deleted.

### Edge Cases

- What happens when a prompt template file has malformed YAML frontmatter (e.g., missing delimiters, invalid YAML syntax)? The parser should produce a clear error message identifying the file and the parsing issue.
- What happens when a prompt template references a Liquid variable that is not provided in the arguments dictionary? The renderer should produce an empty string for that variable (standard Liquid behavior) without throwing an exception.
- What happens when a prompt template contains no role markers (e.g., only raw text with no `system:`, `user:`, or `assistant:` delimiters)? The parser should treat the entire body as a single user message.
- What happens when the AI service returns an empty or null response? The provider should handle this gracefully, logging a warning and returning an appropriate default or error indication.
- What happens when the AI service is rate-limited (HTTP 429)? The client should apply retry policies with exponential backoff, consistent with current behavior.
- What happens when structured output from the AI service does not match the expected JSON schema? The provider should produce a clear error message indicating deserialization failure.
- What happens when the vector store in-memory component has a hard dependency on Semantic Kernel core packages? Research confirmed that `Microsoft.SemanticKernel.Connectors.InMemory` has NO dependency on Semantic Kernel core — its only dependencies are `Microsoft.Extensions.AI.Abstractions`, `Microsoft.Extensions.DependencyInjection.Abstractions`, `Microsoft.Extensions.VectorData.Abstractions`, and `System.Numerics.Tensors`. The package is retained as-is, and existing `SkInMemoryVectorIndexWriter`/`SkInMemoryVectorSearchService` continue working without SK core.

## Requirements *(mandatory)*

### Functional Requirements

- **FR-001**: System MUST replace all Semantic Kernel-based AI invocation with equivalent functionality using Microsoft Agent Framework abstractions (`IChatClient`, `ChatMessage`, `IEmbeddingGenerator` from `Microsoft.Extensions.AI`).
- **FR-002**: System MUST implement a custom prompt template parser that loads files containing YAML frontmatter and role-delimited messages (system, user, assistant), extracting metadata and an ordered list of template messages.
- **FR-003**: System MUST implement a Liquid template renderer capable of processing `{{variable}}` substitutions and `{% for item in collection %}` loop constructs, producing fully rendered chat messages with correct role assignments.
- **FR-004**: System MUST provide a chat client factory that creates AI chat clients for both free-text generation and structured output (JSON schema) use cases.
- **FR-005**: System MUST provide an embedding generator factory that creates embedding generators for vector embedding operations without requiring Semantic Kernel.
- **FR-006**: System MUST support both Entra ID (managed identity via `DefaultAzureCredential`) and API key authentication for connecting to Azure AI services.
- **FR-007**: System MUST track and log token usage (input tokens, output tokens) for all AI operations via `ChatResponse.Usage` properties.
- **FR-008**: System MUST retain all 6 existing prompt template file contents unchanged — only the file extension (`.prompty` → `.prompt`) and directory location (`Prompty/` → `PromptTemplates/`) change.
- **FR-009**: System MUST preserve all existing CLI commands, arguments, output formats, and exit codes without user-visible changes.
- **FR-017**: System MUST provide a migration guide documenting new required fields in `settings.json` for Microsoft Foundry Project configuration, including the Foundry project endpoint (`https://<resource>.services.ai.azure.com/api/projects/<project-name>`).
- **FR-010**: System MUST produce AI-generated descriptions for tables, views, and stored procedures by sending the same rendered prompts to the same model as the current implementation. "Equivalent quality" means the identical prompt is delivered with the same model parameters; the response must be structurally valid but text variation between runs is expected and acceptable.
- **FR-011**: System MUST produce structured AI output (e.g., `TableList`, `TableDataDictionary`) by specifying a JSON response format and correctly deserializing the result.
- **FR-012**: System MUST apply retry policies for transient AI service errors (HTTP 429 rate limiting, 5xx server errors) with exponential backoff (10 retries via `Microsoft.Extensions.Http.Resilience`).
- **FR-013**: System MUST remove all Semantic Kernel NuGet package dependencies from production and test projects after migration, EXCEPT `Microsoft.SemanticKernel.Connectors.InMemory` which has no dependency on Semantic Kernel core and provides the `InMemoryVectorStore` implementation.
- **FR-018**: System MUST retain `Microsoft.SemanticKernel.Connectors.InMemory` as the InMemory vector store implementation. Research confirmed this package has no dependency on Semantic Kernel core libraries and provides full `Microsoft.Extensions.VectorData` compliance.
- **FR-014**: System MUST delete the `ISemanticKernelFactory` interface and `SemanticKernelFactory` implementation and all associated `using` statements and warning suppressions.
- **FR-015**: System MUST register all new services (chat client factory, prompt template parser, Liquid template renderer) in the dependency injection container as singletons.
- **FR-016**: System MUST produce correct rendered output for prompt templates containing few-shot examples with multiple user/assistant message pairs in sequence.
- **FR-019**: System MUST wrap AI operations (chat completions, structured output, embeddings) in structured logging scopes via `ILogger.BeginScope(...)` including operation context (template name, model deployment, operation type) to preserve the existing diagnostic capability mandated by Constitution Principle II.

### Key Entities

- **Prompt Template Definition**: Represents a parsed prompt file, containing metadata (name, description, model parameters such as temperature) and an ordered list of prompt template messages.
- **Prompt Template Message**: Represents a single message within a prompt template, characterized by a role (system, user, or assistant) and content text that may contain Liquid template syntax.
- **Prompt Template Model Parameters**: Represents AI model configuration extracted from YAML frontmatter, including temperature, top_p, and max_tokens.
- **Chat Client Factory**: A service responsible for creating AI chat clients and embedding generators from project configuration, replacing the current Semantic Kernel factory.

## Success Criteria *(mandatory)*

### Measurable Outcomes

- **SC-001**: All existing unit tests pass after migration with updated mocks and assertions — zero test regressions.
- **SC-002**: All 6 prompt templates produce character-for-character identical rendered output when given the same input variables as the previous implementation.
- **SC-003**: All CLI commands (`init-project`, `extract-model`, `enrich-model`, `query-model`, `export-model`, `data-dictionary`, `generate-vectors`, `show-object`) execute successfully with unchanged behavior.
- **SC-004**: Token usage statistics (input tokens, output tokens) are tracked and logged for every AI operation.
- **SC-005**: No Semantic Kernel NuGet packages remain in any project file after migration, except `Microsoft.SemanticKernel.Connectors.InMemory` (retained per FR-018 — no SK core dependency).
- **SC-006**: Both Entra ID and API key authentication paths complete AI operations successfully when configured.
- **SC-007**: Transient AI service errors (HTTP 429, 5xx) are retried with backoff — the application does not crash on recoverable errors.
- **SC-008**: New unit tests achieve coverage for the prompt template parser, Liquid renderer, and chat client factory covering normal operation, edge cases, and error conditions.

## Clarifications

### Session 2026-02-20

- Q: Should settings.json remain fully backward-compatible (no new required fields), or may new required fields be added for Azure AI Foundry? → A: New required fields allowed with migration guide provided.
- Q: Strategy for InMemory vector store (used for local/dev vector indexing and search): drop, replace with non-SK alternative, or keep? → A: Replace with a non-SK alternative to keep the InMemory provider functional without Semantic Kernel dependencies.
- Q: Are existing semantic model files (enriched descriptions, generated vectors) compatible post-migration, or must users re-run enrichment/generation? → A: Fully compatible, no rework required.
- Q: Is the query-model pipeline (embedding search + SQL generation via chat) sufficiently covered by existing user stories, or does it need a dedicated scenario? → A: Existing coverage is sufficient; query-model is a composition of already-covered capabilities.
- Q: What does 'equivalent quality' mean for AI-generated descriptions (FR-010), given AI outputs are non-deterministic? → A: Structural equivalence — same prompt sent to same model and response is structurally valid. Text differences between runs are expected and acceptable.

### Session 2026-02-21

- Q: What strategy should be used for the InMemory vector store — replace with custom non-SK implementations, keep the SK InMemory connector, or use another first-party package? → A: Keep `Microsoft.SemanticKernel.Connectors.InMemory`. Research confirmed it has zero dependency on SK core (only depends on `Microsoft.Extensions.*` packages). No alternative Microsoft first-party InMemory VectorData package exists. The existing Sk* wrappers and adapters only import from the InMemory connector namespace, not SK core.
- Q: SC-005 states "No SK packages remain" but FR-013/FR-018 retain the InMemory connector — how to resolve this contradiction? → A: Amend SC-005 to explicitly exclude `Microsoft.SemanticKernel.Connectors.InMemory` from the removal criterion.
- Q: Should migrated AI operations preserve structured logging scopes (`ILogger.BeginScope`) or is basic call-site logging sufficient? → A: Preserve structured logging scopes — new providers must wrap AI calls in `ILogger.BeginScope(...)` with operation context (template name, model deployment, etc.), per Constitution Principle II.
- Q: US7 Acceptance Scenario 2 says "no `using Microsoft.SemanticKernel` namespaces found" but retained InMemory connector files will still have that import — should it be scoped? → A: Yes, amend to exclude `Microsoft.SemanticKernel.Connectors.InMemory` in vector store files.
- Q: FR-017 and assumptions reference stale "Azure AI Foundry" terminology and R1 rejected `Azure.AI.Projects` — how should the project align given Azure AI Foundry is now Microsoft Foundry? → A: Full Foundry SDK alignment. Add `Azure.AI.Projects` + `Azure.AI.Projects.OpenAI`. `ChatClientFactory` uses `AIProjectClient` with Foundry project endpoint to discover OpenAI connection, then creates `IChatClient`/`IEmbeddingGenerator`. Settings.json uses Foundry project endpoint. Replaces direct `Azure.AI.OpenAI` initialization.

## Assumptions

- Existing semantic model files (`semanticmodel.json`), enriched descriptions, and generated vector files are fully compatible post-migration. No re-enrichment or re-generation is required.
- All 6 prompt templates use only basic Liquid features (`{{variable}}` substitution and `{% for %}` loops). No filters, conditionals, or advanced Liquid constructs are used.
- The Scriban library's Liquid-compatible mode (`ParseLiquid`) produces identical output to the Fluid library used internally by Semantic Kernel's Liquid prompt template package for the template features in use.
- The `Microsoft.Extensions.AI.IChatClient` abstraction supports structured output via JSON schema response format, enabling the existing `TableList` and `TableDataDictionary` deserialization pattern.
- The existing `settings.json` configuration structure will require new required fields for the Microsoft Foundry Project SDK (`Azure.AI.Projects`), specifically a Foundry project endpoint (`https://<resource>.services.ai.azure.com/api/projects/<project-name>`). A migration guide documenting the required changes will be provided.
- The `Microsoft.SemanticKernel.Connectors.InMemory` package has been confirmed to have NO dependency on Semantic Kernel core libraries. It depends only on `Microsoft.Extensions.AI.Abstractions`, `Microsoft.Extensions.DependencyInjection.Abstractions`, `Microsoft.Extensions.VectorData.Abstractions`, and `System.Numerics.Tensors`. It is retained as a standalone dependency.
- The Microsoft Foundry SDK (`Azure.AI.Projects` + `Azure.AI.Projects.OpenAI`) provides `AIProjectClient` which discovers OpenAI connections from a single project endpoint and creates `AzureOpenAIClient` instances, which in turn provide `IChatClient` and `IEmbeddingGenerator` implementations compatible with the deployed Azure OpenAI models.
- Azure OpenAI Service is now part of Microsoft Foundry and is accessed via Foundry Project endpoints. The application aligns to this architecture.

## Constraints

- The public CLI interface (commands, options, output) MUST NOT change.
- The application MUST continue targeting .NET 10.
- Microsoft Agent Framework is currently in preview (`1.0.0-rc1`); the API may change before GA, but the application primarily uses stable `Microsoft.Extensions.AI` types.
- Bicep infrastructure changes for Microsoft Foundry are out of scope for this feature and handled separately.

## Dependencies

- .NET 10 SDK
- `Microsoft.Extensions.AI` package (stable)
- `Microsoft.Extensions.AI.OpenAI` package
- `Azure.AI.OpenAI` package (Azure OpenAI client, provides `AzureOpenAIClient` implementing `IChatClient`)
- `Azure.AI.Projects` package (Microsoft Foundry SDK — provides `AIProjectClient` for project endpoint connection discovery)
- `Azure.AI.Projects.OpenAI` package (Foundry SDK OpenAI integration — bridges `AIProjectClient` to `AzureOpenAIClient`)
- `Scriban` NuGet package (stable, actively maintained)
- `YamlDotNet` NuGet package (stable, widely used)
- Microsoft Foundry project with Azure OpenAI model deployments (for integration testing)

> **Note**: Research decision R1 originally rejected `Azure.AI.Projects` in favor of direct `Azure.AI.OpenAI`. This was revised during clarification: Azure AI Foundry is now Microsoft Foundry, and Azure OpenAI Service is exposed via Foundry Project endpoints. The application now aligns to the Foundry Project architecture using `Azure.AI.Projects` + `Azure.AI.Projects.OpenAI`, with `AIProjectClient` discovering OpenAI connections from a single project endpoint.

- `Microsoft.SemanticKernel.Connectors.InMemory` package (retained — no SK core dependency; provides `InMemoryVectorStore` implementing `Microsoft.Extensions.VectorData` interfaces)
- Existing project dependencies (System.CommandLine, FluentAssertions, Moq) must remain compatible

## Risks

- **RISK-001**: Microsoft Agent Framework is preview — API may change before GA. Mitigated by primarily using stable `Microsoft.Extensions.AI` types.
- **RISK-002**: Scriban's Liquid-compatible mode may differ subtly from Fluid (used by SK) for edge-case template syntax. Mitigated by the fact that only basic Liquid features are used, and `Prompty.Core` itself uses Scriban.
- **RISK-003**: Microsoft Foundry Project SDK (`Azure.AI.Projects`) configuration model differs from the current `OpenAIServiceSettings` — a Foundry project endpoint replaces direct Azure OpenAI endpoint configuration. Mitigated by designing the `ChatClientFactory` to use `AIProjectClient` for connection discovery and mapping existing settings fields with a documented migration guide per FR-017.
- **RISK-004**: ~~The InMemory vector store replacement must maintain functional parity.~~ **RESOLVED**: Research confirmed `Microsoft.SemanticKernel.Connectors.InMemory` has no SK core dependency. Package is retained — no replacement needed. Existing Sk* wrappers and adapters compile without SK core packages.
- **RISK-005**: Token usage tracking format may differ between SK and `Microsoft.Extensions.AI`. Mitigated by mapping the new format to existing tracking structures during implementation.
